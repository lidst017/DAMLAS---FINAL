{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAMLAS MidTerm Exam  \n",
    "\n",
    "11:00AM - 1:00PM(CT)\n",
    "August 22, 2016   \n",
    "End of Term Exam\n",
    "\n",
    "\n",
    "Data Analytics and Machine Learning at Scale\n",
    "\n",
    "Target, Minneapolis\n",
    "\n",
    "### Please insert your contact information here\n",
    "Mark Lidstone  \n",
    "mark.lidstone@target.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions for exam \n",
    "\n",
    "0. Please submit your solutions and notebook via the following form:\n",
    "\n",
    "     [Exam Submission form](http://goo.gl/forms/iN4XIXQyiH1iAdyD3) \n",
    "\n",
    "            \n",
    "1. __Please acknowledge receipt of exam by sending a quick reply to the Jimi__ \n",
    "2. Review the submission form first to scope it out (it will take a 5-10 minutes to input your \n",
    "   answers and other information into this form)\n",
    "3. Please keep all your work and responses in ONE (1) notebook only (and submit via the submission form)\n",
    "4. Please make sure that the NBViewer link for your Submission notebook works\n",
    "5. Please do NOT discuss this exam with anyone (including your class mates) __until after Monday, September 5. __\n",
    "6. This is an open book exam meaning you can consult webpages and textbooks, class notes, slides etc. but you can not consult each other or any other person/group. Please complete this exam by yourself within the time limit. \n",
    "7. For markdown help in iPython Notebooks please see:\n",
    "https://sourceforge.net/p/ipython/discussion/markdown_syntax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam questions begins here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:1\n",
    "Assume your tasked with modeling a REGRESSION problem. How do you determine which variables may be important?\n",
    "\n",
    "\n",
    "1. If your data has unknown structure, start with:  Tree-based methods\n",
    "+ If statistical measures of importance are needed, start with: linear models (think Generalized linear models (GLMs)\n",
    "+ If statistical measures of importance are not needed, start with: Regression with shrinkage (e.g., LASSO, Elastic net)\n",
    "+ If statistical measures of importance are not needed, use : Stepwise regression\n",
    "\n",
    "Select the single most correct response from the following: \n",
    "\n",
    "+ (a) 1 \n",
    "+ (b) 2\n",
    "+ (c) 3\n",
    "+ (d) 1, 2, 3, 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**   D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:2\n",
    "Using one-hot-encoding, a categorical feature with four distinct values would be represented by how many features?\n",
    "\n",
    "(a) 1 feature  \n",
    "(b) 2 features  \n",
    "(c) 4 features  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**  D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:3\n",
    "When it comes to decision tree prediction (in the case of either classification or regression problems) which of the the following steps are perfermed:\n",
    "\n",
    "1. A decision tree takes as input an object or situation described by a set of attributes and returns a decision the predicted out value for the input.\n",
    "2. A decision tree reaches its decision by performing a sequence of tests.\n",
    "3. Each internal node in the tree corresponds to a test of the value of one of the properties, and the branches from the node are labeled with the possible values of the test.\n",
    "4. Each leaf node in the tree specifies the value to be returned if that leaf is reached.\n",
    "\n",
    "+ (a) 1 \n",
    "+ (b) 1, 2, 3, 4\n",
    "+ (c) 3\n",
    "+ (d) 1, 2, 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:4\n",
    "\n",
    "Using Claude Shannon’s model of Entropy, compute the: \n",
    "\n",
    "+ Entropy of a fair coin\n",
    "+ Entropy of an unfair coint that comes up heads 80% of the time\n",
    "\n",
    "Select the correct answer from the following:\n",
    "\n",
    "+ (a) 1 bit (entropy of a fair coin) and 0.8 bits (the biased coin)\n",
    "+ (b) 1 bit (entropy of a fair coin) and 1 bit (the biased coin)\n",
    "+ (c) 1 bit (entropy of a fair coin) and 0.2068 bits (the biased coin)\n",
    "+ (d) 1 bit (entropy of a fair coin) and 0.2 bits (the biased coin)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:  C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:5\n",
    "Given the following gradient descent scenario indicate which of the statements below are correct: \n",
    "\n",
    " + Given an objective function (also know as a cost fucntion) that we wish to minimize. This function has one decision variable w1.\n",
    " + Suppose we reach a local minimum, i.e.,  w1 is already at a local minimum, what do you think one step of gradient descent will do? \n",
    " \n",
    " \n",
    " __STATEMENTS__\n",
    "\n",
    "1. It turns out at the local optimum, your derivative will be equal to zero. \n",
    "2. So  the slope of the line  tangent to the objective function at this point w1  will be equal to zero and thus this derivative term is equal to zero. \n",
    "3. The  gradient descent update at this local optimum is, w1 = w1 -  alpha * 0; where alpha is the learning rate \n",
    "4. If you're already at the local optimum it leaves  w1 unchanged cause its updates as w1 equals w1. So if your parameters are already at a local minimum one step with gradient descent does absolutely nothing. It keeps your solution at the local optimum. \n",
    "\n",
    "\n",
    "Select the single most correct answer from the following:\n",
    "\n",
    "+ (a) 1, 2, 3, 4\n",
    "+ (b) 2\n",
    "+ (c) 3,4\n",
    "+ (d) 2,3,4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:6  \n",
    "\n",
    "Given the following gradient descent scenario indicate which of the statements below are correct: \n",
    "\n",
    " + Given a convex objective function (also known as a cost fucntion) that we wish to minimize. This function has one decision variable w1.\n",
    " + Suppose we reach a local minimum, i.e.,  w1 is already at a local minimum, what do you think one step of gradient descent will do? \n",
    " + use a fixed learning rate alpha; assume alpha is 1. \n",
    " \n",
    "__STATEMENTS__  \n",
    "a. When the current estimate of the minimum of the objective function is further away from the mimimum the gradient is much bigger   \n",
    "b. When the current estimate of the minimum of the objective function is close to the mimimum, my derivative term is even smaller and so the magnitude of the update to w1 is even smaller.  \n",
    "c. As gradient descent runs, you will automatically take smaller and smaller steps. Until eventually you're taking very small steps, and you finally converge to the  global minimum (it is a convex optimization after all).     \n",
    "d. Gradient descent will never converge will a fixed learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: A, B, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:7\n",
    "When dealing with numercial data which of the following are ways to deal with missing data:\n",
    "\n",
    "(a) Delete records that have missing input values  \n",
    "(b) Standardize the data and set all missing values to 1 (one)  \n",
    "(c) Use K-nearest neighbours based on the test set to fill in missing values in the training set  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:8\n",
    "In a digital advertising problem where we are modeling the Pr(Click|context) and have no downstream knowledge of what purchases resulted from a click on the ad shown, which of the following approaches would lead to potentially more clicks on the ads shown?\n",
    "\n",
    "(a) Model Click vs not click event using linear regression   \n",
    "(b) Model Click vs not click event using logistic regression  \n",
    "(c) Model probability of a purchase  \n",
    "(d) none of the above  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:9\n",
    "Which of the following are true about the purpose of a loss function?\n",
    "\n",
    "(a) It’s a way to penalize a model for incorrect predictions  \n",
    "(b) It precisely defines the optimization problem to be solved for a particular learning model  \n",
    "(c) Loss functions can be used for modeling both classification and regression problems  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: B,C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:10\n",
    "When implementing Logistic (or linear) Regression with Regularization in Spark which of the following apply when using the following cost function: \n",
    "\n",
    "\\begin{equation*}\n",
    "minimize   \\left(f(w):= λR(w) + \\sum_{k=1}^n \\left(w;w_i,y_i \\right) \\right) \n",
    "\\end{equation*}\n",
    "\n",
    "(a) When lambda equals one, it provides the same result as standard logistic (linear) regression   \n",
    "(b) One only needs to modify the standard logistic (linear) regression (i.e., with no regularization term) by adding some code after the map-reduce (loss) gradient steps    \n",
    "(c) When lambda equals zero, it provides the same result as standard logistic (linear) regression  \n",
    "(d) One only needs to modify the standard logistic (linear) regression by modifying the mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:11 Given the following paired RDDs \n",
    "RDD1 = {(1, 2), (3, 4), (3, 6)}\n",
    "RDD2 = {(3, 9) (3, 6)}\n",
    "\n",
    "Using PySpark, write code to perform an inner join of these paired RDDs. What is the resulting RDD? Make your Spark available in your notebook:\n",
    "\n",
    "a: [(3, (4, 9)), (3, (6, 9))]    \n",
    "b: [(3, (4, 9)), (3, (4, 6)), (3, (6, 9)), (3, (6, 6))]   \n",
    "c: [(3, (4, 9)), (3, (4, 6)), (3, (6, 9)), (3, (6, 9))]    \n",
    "d: None of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version a but this version of numpy is 9",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version a but this version of numpy is 9"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x107f93350>\n",
      "<pyspark.sql.context.SQLContext object at 0x108281b90>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys #current as of 9/26/2015\n",
    "\n",
    "# spark_home = os.environ['SPARK_HOME'] = '/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/spark-1.6.1-bin-hadoop2.6/'\n",
    "spark_home = os.environ['SPARK_HOME'] = '/Users/z013l75/spark-1.6.2-bin-hadoop2.6/'\n",
    "\n",
    "\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.9-src.zip'))\n",
    "\n",
    "# First, we initialize the Spark environment\n",
    "\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# We can give a name to our app (to find it in Spark WebUI) and configure execution mode\n",
    "# In this case, it is local multicore execution with \"local[*]\"\n",
    "app_name = \"example-logs\"\n",
    "master = \"local[*]\"\n",
    "conf = pyspark.SparkConf().setAppName(app_name).setMaster(master)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "print(sc)\n",
    "print(sqlContext)\n",
    "\n",
    "\n",
    "# Import some libraries to work with dates\n",
    "import dateutil.parser\n",
    "import dateutil.relativedelta as dateutil_rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, (4, 9)), (3, (4, 6)), (3, (6, 9)), (3, (6, 6))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD1= sc.parallelize([(1, 2), (3, 4), (3, 6)])\n",
    "\n",
    "RDD2= sc.parallelize([(3, 9), (3, 6)])\n",
    "\n",
    "superrdd = RDD1.join(RDD2)\n",
    "superrdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:12  You have been tasked to build a predictive model to forecast beer sales for a chain of stores.\n",
    "\n",
    "\n",
    "After doing basic exploratory analysis on the data, what is the first thing you do regarding modeling?\n",
    "\n",
    "\n",
    "(a) Construct a baseline model  \n",
    "(b) Determine a metric to evaluate your machine learnt models  \n",
    "(c) Split your data into training, validation and test subsets (or split using cross fold validatation)  \n",
    "(d) All of the  of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:13\n",
    "\n",
    "The beer sales data consists of 52 weeks of cases-sold and price-per-case data for 3 carton sizes of beer (12-packs, 18-packs, 30-packs) at a small chain of supermarkets.\n",
    "\n",
    "Three additional rows of hypothetical price data for 12-,18-, 30-packs have been entered for purposes of forecasting from the models.  \n",
    "\n",
    "\n",
    "Use Spark and the following notebook, https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.\n",
    "\n",
    "The mean absolute percentage error (MAPE), also known as mean absolute percentage deviation (MAPD), is a measure of prediction accuracy of a model for say a forecasting method in statistics, \n",
    "for example in trend estimation. It usually expresses accuracy as a percentage, and is defined by the formula:\n",
    "\n",
    "MAPE = average over all examples (100*Abs(Actual - Predicted) / Actual)) \n",
    "\n",
    "Note when Actual is zero that test row is dropped from the evaluation.\n",
    "\n",
    "Construct a mean model for target variable `CASES18PK` (for the purposes of this question, take the mean of the CASES18PK column as your model prediction). Calculate the MAPE for the mean model over the training set. Select the closest answer to your calculated MAPE.\n",
    "\n",
    "(a) 200%  \n",
    "(b) 250%  \n",
    "(c) 20%  \n",
    "(d) 180%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing beerSales.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile beerSales.txt\n",
    "Week\tPRICE12PK\tPRICE18PK\tPRICE30PK\tCASES12PK\tCASES18PK\tCASES30PK\n",
    "1\t19.98\t14.10\t15.19\t223.5\t439\t55.00\n",
    "2\t19.98\t18.65\t15.19\t215.0\t98\t66.75\n",
    "3\t19.98\t18.65\t13.87\t227.5\t70\t242.00\n",
    "4\t19.98\t18.65\t12.83\t244.5\t52\t488.50\n",
    "5\t19.98\t18.65\t13.16\t313.5\t64\t308.75\n",
    "6\t19.98\t18.65\t15.19\t279.0\t72\t111.75\n",
    "7\t19.98\t18.65\t13.92\t238.0\t47\t252.50\n",
    "8\t20.10\t18.73\t14.42\t315.5\t85\t221.25\n",
    "9\t20.12\t18.75\t13.83\t217.0\t59\t245.25\n",
    "10\t20.13\t18.75\t14.50\t209.5\t63\t148.50\n",
    "11\t20.14\t18.75\t13.87\t227.0\t57\t229.75\n",
    "12\t20.12\t18.75\t13.64\t216.5\t54\t312.00\n",
    "13\t20.12\t13.87\t14.31\t169.0\t404\t96.75\n",
    "14\t20.13\t14.27\t13.85\t178.0\t380\t123.25\n",
    "15\t20.14\t18.76\t14.20\t301.5\t65\t200.50\n",
    "16\t20.14\t18.77\t13.64\t266.5\t40\t359.75\n",
    "17\t20.13\t13.87\t14.33\t182.5\t456\t113.50\n",
    "18\t20.13\t14.14\t13.14\t159.0\t176\t136.50\n",
    "19\t20.13\t18.76\t13.81\t285.5\t61\t225.50\n",
    "20\t20.13\t18.72\t15.19\t360.0\t91\t122.25\n",
    "21\t20.13\t18.76\t13.13\t263.0\t59\t443.75\n",
    "22\t19.18\t18.76\t13.63\t443.5\t83\t322.75\n",
    "23\t14.78\t18.74\t15.19\t1101.5\t41\t53.00\n",
    "24\t16.04\t18.75\t13.89\t814.0\t47\t140.75\n",
    "25\t20.12\t18.75\t14.28\t365.0\t84\t210.75\n",
    "26\t19.75\t18.75\t15.19\t510.0\t85\t110.50\n",
    "27\t19.65\t18.75\t13.12\t580.5\t116\t568.25\n",
    "28\t19.69\t13.79\t13.78\t251.0\t544\t115.50\n",
    "29\t20.12\t13.49\t15.19\t237.0\t890\t58.75\n",
    "30\t20.12\t14.89\t15.19\t302.5\t371\t77.25\n",
    "31\t20.13\t13.94\t15.19\t229.5\t557\t66.25\n",
    "32\t20.14\t13.67\t15.19\t188.5\t775\t50.00\n",
    "33\t15.14\t14.43\t15.19\t795.5\t236\t46.50\n",
    "34\t14.33\t18.75\t15.19\t1556.5\t43\t65.75\n",
    "35\t16.24\t18.22\t13.14\t807.5\t63\t252.75\n",
    "36\t19.93\t14.06\t13.45\t243.0\t469\t179.00\n",
    "37\t21.06\t14.43\t13.00\t201.5\t335\t226.25\n",
    "38\t21.19\t19.48\t13.60\t294.0\t75\t288.50\n",
    "39\t21.23\t15.15\t14.46\t220.5\t461\t114.25\n",
    "40\t20.12\t13.79\t14.94\t255.5\t817\t70.00\n",
    "41\t14.73\t14.31\t15.19\t920.5\t200\t47.75\n",
    "42\t14.57\t19.50\t15.19\t730.0\t32\t98.75\n",
    "43\t15.94\t13.85\t15.19\t262.5\t460\t77.00\n",
    "44\t20.70\t14.23\t13.43\t209.5\t751\t160.50\n",
    "45\t19.57\t19.31\t14.37\t283.0\t70\t143.50\n",
    "46\t19.60\t19.29\t15.19\t262.5\t80\t133.00\n",
    "47\t19.94\t13.76\t15.19\t310.0\t523\t68.75\n",
    "48\t21.28\t13.45\t15.19\t278.5\t741\t81.75\n",
    "49\t14.56\t15.13\t15.19\t741.5\t130\t56.25\n",
    "50\t14.39\t19.43\t15.19\t1316.0\t69\t68.75\n",
    "51\t16.81\t13.26\t15.19\t449.0\t493\t49.25\n",
    "52\t19.86\t13.92\t15.19\t505.0\t814\t76.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-875b63826c9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# spark is an existing SparkSession.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load a text file and convert each line to a Row.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession.\n",
    "from pyspark.sql import Row\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"beerSales.txt\")\n",
    "parts = lines.map(lambda l: l.split(\" \"))\n",
    "people = parts.map(lambda p: Row(Week=p[0],PRICE12PK=int(p[]) PRICE18PK PRICE30PK CASES12PK CASES18PK CASES30PK \n",
    "                                 CASES18PK=int(p[5])))\n",
    "\"\"\"\n",
    "#PRICE12PK\tPRICE18PK\tPRICE30PK\tCASES12PK\tCASES18PK\tCASES30PK\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
    "teenNames = teenagers.map(lambda p: \"Name: \" + p.name)\n",
    "for teenName in teenNames.collect():\n",
    "  print(teenName)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Week',\n",
       "  u'PRICE12PK',\n",
       "  u'PRICE18PK',\n",
       "  u'PRICE30PK',\n",
       "  u'CASES12PK',\n",
       "  u'CASES18PK',\n",
       "  u'CASES30PK'],\n",
       " [u'1', u'19.98', u'14.10', u'15.19', u'223.5', u'439', u'55.00'],\n",
       " [u'2', u'19.98', u'18.65', u'15.19', u'215.0', u'98', u'66.75'],\n",
       " [u'3', u'19.98', u'18.65', u'13.87', u'227.5', u'70', u'242.00'],\n",
       " [u'4', u'19.98', u'18.65', u'12.83', u'244.5', u'52', u'488.50'],\n",
       " [u'5', u'19.98', u'18.65', u'13.16', u'313.5', u'64', u'308.75'],\n",
       " [u'6', u'19.98', u'18.65', u'15.19', u'279.0', u'72', u'111.75'],\n",
       " [u'7', u'19.98', u'18.65', u'13.92', u'238.0', u'47', u'252.50'],\n",
       " [u'8', u'20.10', u'18.73', u'14.42', u'315.5', u'85', u'221.25'],\n",
       " [u'9', u'20.12', u'18.75', u'13.83', u'217.0', u'59', u'245.25'],\n",
       " [u'10', u'20.13', u'18.75', u'14.50', u'209.5', u'63', u'148.50'],\n",
       " [u'11', u'20.14', u'18.75', u'13.87', u'227.0', u'57', u'229.75'],\n",
       " [u'12', u'20.12', u'18.75', u'13.64', u'216.5', u'54', u'312.00'],\n",
       " [u'13', u'20.12', u'13.87', u'14.31', u'169.0', u'404', u'96.75'],\n",
       " [u'14', u'20.13', u'14.27', u'13.85', u'178.0', u'380', u'123.25'],\n",
       " [u'15', u'20.14', u'18.76', u'14.20', u'301.5', u'65', u'200.50'],\n",
       " [u'16', u'20.14', u'18.77', u'13.64', u'266.5', u'40', u'359.75'],\n",
       " [u'17', u'20.13', u'13.87', u'14.33', u'182.5', u'456', u'113.50'],\n",
       " [u'18', u'20.13', u'14.14', u'13.14', u'159.0', u'176', u'136.50'],\n",
       " [u'19', u'20.13', u'18.76', u'13.81', u'285.5', u'61', u'225.50'],\n",
       " [u'20', u'20.13', u'18.72', u'15.19', u'360.0', u'91', u'122.25'],\n",
       " [u'21', u'20.13', u'18.76', u'13.13', u'263.0', u'59', u'443.75'],\n",
       " [u'22', u'19.18', u'18.76', u'13.63', u'443.5', u'83', u'322.75'],\n",
       " [u'23', u'14.78', u'18.74', u'15.19', u'1101.5', u'41', u'53.00'],\n",
       " [u'24', u'16.04', u'18.75', u'13.89', u'814.0', u'47', u'140.75'],\n",
       " [u'25', u'20.12', u'18.75', u'14.28', u'365.0', u'84', u'210.75'],\n",
       " [u'26', u'19.75', u'18.75', u'15.19', u'510.0', u'85', u'110.50'],\n",
       " [u'27', u'19.65', u'18.75', u'13.12', u'580.5', u'116', u'568.25'],\n",
       " [u'28', u'19.69', u'13.79', u'13.78', u'251.0', u'544', u'115.50'],\n",
       " [u'29', u'20.12', u'13.49', u'15.19', u'237.0', u'890', u'58.75'],\n",
       " [u'30', u'20.12', u'14.89', u'15.19', u'302.5', u'371', u'77.25'],\n",
       " [u'31', u'20.13', u'13.94', u'15.19', u'229.5', u'557', u'66.25'],\n",
       " [u'32', u'20.14', u'13.67', u'15.19', u'188.5', u'775', u'50.00'],\n",
       " [u'33', u'15.14', u'14.43', u'15.19', u'795.5', u'236', u'46.50'],\n",
       " [u'34', u'14.33', u'18.75', u'15.19', u'1556.5', u'43', u'65.75'],\n",
       " [u'35', u'16.24', u'18.22', u'13.14', u'807.5', u'63', u'252.75'],\n",
       " [u'36', u'19.93', u'14.06', u'13.45', u'243.0', u'469', u'179.00'],\n",
       " [u'37', u'21.06', u'14.43', u'13.00', u'201.5', u'335', u'226.25'],\n",
       " [u'38', u'21.19', u'19.48', u'13.60', u'294.0', u'75', u'288.50'],\n",
       " [u'39', u'21.23', u'15.15', u'14.46', u'220.5', u'461', u'114.25'],\n",
       " [u'40', u'20.12', u'13.79', u'14.94', u'255.5', u'817', u'70.00'],\n",
       " [u'41', u'14.73', u'14.31', u'15.19', u'920.5', u'200', u'47.75'],\n",
       " [u'42', u'14.57', u'19.50', u'15.19', u'730.0', u'32', u'98.75'],\n",
       " [u'43', u'15.94', u'13.85', u'15.19', u'262.5', u'460', u'77.00'],\n",
       " [u'44', u'20.70', u'14.23', u'13.43', u'209.5', u'751', u'160.50'],\n",
       " [u'45', u'19.57', u'19.31', u'14.37', u'283.0', u'70', u'143.50'],\n",
       " [u'46', u'19.60', u'19.29', u'15.19', u'262.5', u'80', u'133.00'],\n",
       " [u'47', u'19.94', u'13.76', u'15.19', u'310.0', u'523', u'68.75'],\n",
       " [u'48', u'21.28', u'13.45', u'15.19', u'278.5', u'741', u'81.75'],\n",
       " [u'49', u'14.56', u'15.13', u'15.19', u'741.5', u'130', u'56.25'],\n",
       " [u'50', u'14.39', u'19.43', u'15.19', u'1316.0', u'69', u'68.75'],\n",
       " [u'51', u'16.81', u'13.26', u'15.19', u'449.0', u'493', u'49.25'],\n",
       " [u'52', u'19.86', u'13.92', u'15.19', u'505.0', u'814', u'76.50']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, min, max\n",
    "from pyspark.mllib.stat import Statistics\n",
    "beerdf=sc.textFile(\"beerSales.txt\")\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "counts = beerdf.map(lambda line: line.split(\"\\t\")) \\\n",
    "  \n",
    "counts.collect()    \n",
    "#avgbeerdf = beerdf.select([mean('CASES18PK')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer**: A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:14\n",
    "Use Spark and the following notebook for the following questions:\n",
    "\n",
    "* http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb\n",
    "* https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.\n",
    "\n",
    "The target variable `CASES18P`K is skewed, so take the log of it (and make it more normally distributed) and compute the MAPE of the mean model for `LN_CASES18PK`. Select the closest answer to your calculated MAPE.\n",
    "\n",
    "(a) 200%  \n",
    "(b) 30%  \n",
    "(c) 20%  \n",
    "(d) 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ET:15\n",
    "Use Spark and the following notebook for the following questions:\n",
    "\n",
    "* http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb\n",
    "* https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.\n",
    "\n",
    "\n",
    "Build a linear regression model using the following variables:\n",
    "\n",
    "Log(CASES18PK)  ~  log(PRICE12PK) + \tlog(PRICE18PK) +\tlog(PRICE30PK)\n",
    "\n",
    "Calculate MAPE over the test set and select the closest answer.\n",
    "\n",
    "(a) 4.3%   \n",
    "(b) 4.6%   \n",
    "(c) 3.5%  \n",
    "(d) 3.9%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET:16\n",
    "Recall that Spark automatically sends all variables referenced in your closures to the\n",
    "worker nodes. While this is convenient, it can also be inefficient because (1) the\n",
    "default task launching mechanism is optimized for small task sizes, and (2) you\n",
    "might, in fact, use the same variable in multiple parallel operations, but Spark will\n",
    "send it separately for each operation. As an example, say that we wanted to write a\n",
    "Spark program that looks up countries by their call signs (e.g., the call sign for Ireland is EJZ) by prefix matching in an\n",
    "table. In the following the \"signPrefixes\" variable is essentially a table with two columns \"Sign\" and \"Country Name\". The goal is \n",
    "to join the following tables:\n",
    "\n",
    "`signPrefixes` table with columns \"Sign\" and \"Country Name\"  \n",
    "`contactCounts` table with columns \"Sign\" and \"count\"\n",
    "\n",
    "to yield  a new table:\n",
    "\n",
    "`countryContactCounts` with the following columns \"Country Name\" and \"count\"\n",
    "\n",
    "Use Spark and the following notebook for the following questions:\n",
    "\n",
    "* http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb\n",
    "* https://www.dropbox.com/s/6s5ph41h74bggwi/Linear-Regression-on-Beer-Data.ipynb?dl=0 to answer this question.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'callsign_tbl_sorted.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-d371e27a00e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m## prefixes to country code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#to support this lookup.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msignPrefixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadCallSignTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-d371e27a00e5>\u001b[0m in \u001b[0;36mloadCallSignTable\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadCallSignTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"callsign_tbl_sorted.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'callsign_tbl_sorted.txt'"
     ]
    }
   ],
   "source": [
    "#..... Other code...\n",
    "#Country lookup code\n",
    "\n",
    "## Helper functions for looking up the call signs\n",
    "\n",
    "def lookupCountry(sign, prefixes):\n",
    "    pos = bisect.bisect_left(prefixes, sign)\n",
    "    return prefixes[pos].split(\",\")[1]\n",
    "\n",
    "\n",
    "def loadCallSignTable():\n",
    "    f = open(\"callsign_tbl_sorted.txt\", \"r\")\n",
    "    return f.readlines()\n",
    "\n",
    "## Lookup the locations of the call signs on the\n",
    "## RDD contactCounts. We load a list of call sign\n",
    "## prefixes to country code\n",
    "#to support this lookup.\n",
    "signPrefixes = loadCallSignTable()\n",
    "\n",
    "\n",
    "def processSignCount(sign_count, signPrefixes):\n",
    "    country = lookupCountry(sign_count[0], signPrefixes)\n",
    "    count = sign_count[1]\n",
    "    return (country, count)\n",
    "\n",
    "countryContactCounts = (contactCounts\n",
    "                        .map(lambda signCount: processSignCount(signCount, signPrefixes))\n",
    "                        .reduceByKey((lambda x, y: x + y)))\n",
    "\n",
    "countryContactCounts.saveAsTextFile(outputDir + \"/countries.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we modfify this code to make it more efficient? Choose one response only\n",
    "\n",
    "(a) modify line 18 with `sc.broadcast(loadCallSignTable())`\n",
    "\n",
    "(b) Use accumulators to store the counts for each country  \n",
    "(c) The code is already optimal  \n",
    "(d) none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# End of Exam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
